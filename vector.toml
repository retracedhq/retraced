data_dir = "/var/lib/vector/"
[api]
  enabled = true
# The data source that Vector will collect logs from
[sources.webhook]
type = "http_server" # The protocol to use
address = "0.0.0.0:9000" # The address to bind to
healthcheck = true # Enable built-in health checks
body_size_limit = "1mb" # Maximum size of request body
auth.password = "${EXPORT_WEBHOOK_PASSWORD}"
auth.username = "${EXPORT_WEBHOOK_USERNAME}"

# The transformation(s) to apply to each event for DataDog
[transforms.add_datadog_info]
type = "remap"
inputs = [ "webhook" ]
source = """
# Set the values of the output object
.ddsource = "${EXPORT_DDSOURCE}"
.ddtags = "${EXPORT_DDTAGS}"
.hostname = "${EXPORT_DATADOG_HOSTNAME}"
.service = "${EXPORT_SERVICE}"
"""

# The transformation(s) to apply to each event for s3
[transforms.s3_transform]
type = "remap"
inputs = [ "webhook" ]
source = """
# Set the values of the output object
. = parse_json!(.message)
"""

[transforms.ecs_transform]
type = "remap"
inputs = [ "webhook" ]
source = """
# Set the values of the output object
. = parse_json!(.message)
.host.name = "localhost"
.host.ip = .source_ip
.event.action = .action
.event.code = .crud
.event.module = .component
if .event.received == null {
  .event.received = now()
} else {
  .event.received = format_timestamp!(.event.received, format: "%+")
}
.event.dataset = "Audit Log"
.user.id = .actor.id
.user.name = .actor.name
.user.domain = .actor.href
.service.id = .target.id
.service.name = .target.name
.service.address = .target.href
.service.type = .target.type
.group.id = .group.id
.group.name = .group.name
.group.domain = .group.href
.source.ip = .source_ip
.message = .description
if .event.is_failure == true {
  .event.outcome = "failure"
  } else {
  .event.outcome = "success"
}
if .event.created == null {
  .@timestamp = now()
} else {
  .@timestamp = format_timestamp!(.event.created, format: "%+")
}
del(.actor)
del(.target)
del(.received)
del(.action)
del(.crud)
del(.is_failure)
del(.component)
del(.group)
del(.source_ip)
del(.description)
del(.created)
del(.canonical_time)
"""

# The destination(s) to send the events to
[sinks.datadog_sink]
type = "datadog_logs"
inputs = [ "add_datadog_info" ]
default_api_key = "${EXPORT_DATADOG_API_KEY}"
compression = "gzip"
region = "${EXPORT_DATADOG_REGION}"
site = "${EXPORT_DATADOG_SITE}"
acknowledgements.enabled = true
healthcheck.enabled = true
request.concurrency = 10
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
# 1GB
buffer.max_size = 1073741952

[sinks.s3_sink]
type = "aws_s3"
inputs = ["s3_transform"]
bucket = "test"
region = "${EXPORT_S3_REGION}"
endpoint = "http://minio:9000"
encoding.codec = "json"
acknowledgements.enabled = true
auth.access_key_id="${EXPORT_S3_ACCESS_KEY_ID}"
auth.secret_access_key="${EXPORT_S3_SECRET_ACCESS_KEY}"
batch.max_events = 1000

[sinks.ecs_sink]
type = "elasticsearch"
inputs = [ "ecs_transform" ]
acknowledgements.enabled = true
api_version = "v7"
auth.strategy = "basic"
auth.user = "elastic"
auth.password = "changeme"
buffer.type = "memory"
buffer.max_events = 10
bulk.action = "index"
bulk.index = "vector-%Y-%m-%d"
endpoints = ["http://elasticsearch:9200"]